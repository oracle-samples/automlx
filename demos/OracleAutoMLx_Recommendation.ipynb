{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333c631f",
   "metadata": {},
   "source": [
    "\n",
    "<h1><font color=red>Building a Recommender using AutoMLx</font></h1>\n",
    "<p style=\"margin-left:5%; margin-right:5%;\">by the <font color=teal> Oracle AutoMLx Team </font></p>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7836ea4d",
   "metadata": {},
   "source": [
    "Recommendation Demo Notebook.\n",
    "\n",
    "Copyright © 2024, Oracle and/or its affiliates.\n",
    "\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7aafc",
   "metadata": {},
   "source": [
    "# Overview of this Notebook\n",
    "\n",
    "In this notebook we will build a recommender using the Oracle AutoMLx tool for the Movielens 100k dataset to predict the next item that users will most likely watch, based on their ratings history.\n",
    "We explore the various options provided by the Oracle AutoMLx tool, allowing the user to control the AutoMLx training process. We finally evaluate the different models trained by AutoMLx. Depending on the machine running this notebook, it can take up to minutes.\n",
    "\n",
    "---\n",
    "## Prerequisites:\n",
    "\n",
    "   - Experience level: Novice (Python and Machine Learning)\n",
    "   - Professional experience: Some industry experience\n",
    "---\n",
    "\n",
    "## Business Use:\n",
    "\n",
    "Data analytics and modeling problems using Machine Learning (ML) are becoming popular and often rely on data science expertise to build accurate ML models. Such modeling tasks primarily involve the following steps:\n",
    "- Preprocess dataset (clean, impute, engineer features, normalize).\n",
    "- Pick an appropriate model for the given dataset and prediction task at hand.\n",
    "- Tune the chosen model’s hyperparameters for the given dataset.\n",
    "\n",
    "All of these steps are significantly time consuming and heavily rely on data scientist expertise. Unfortunately, to make this problem harder, the best feature subset, model, and hyperparameter choice widely varies with the dataset and the prediction task. Hence, there is no one-size-fits-all solution to achieve reasonably good model performance. Using a simple Python API, AutoML can quickly jump-start the datascience process with an accurately-tuned model and appropriate features for a given prediction task.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- <a href='#setup'> Setup</a>\n",
    "- <a href='#load-data'> Load the Movielens 100k dataset</a>\n",
    "  - <a href='#coltypes'> Define the column types</a>\n",
    "  - <a href='#dataset-split'>Splitting the dataset</a>\n",
    "- <a href='#automl'> AutoML</a>\n",
    "  - <a href='#provider'> Create an Instance of AutoMLx</a>\n",
    "  - <a href='#default'> Train a Model using AutoMLx</a>\n",
    "  - <a href='#recommending'> Generate recommendations </a>\n",
    "  - <a href='#analyze'> Analyze the AutoMLx optimization process </a>\n",
    "      - <a href='#algorithm-selection'> Algorithm Selection</a>\n",
    "      - <a href='#hyperparameter-tuning'> Hyperparameter Tuning</a>\n",
    "  - <a href='#modellist'> Advanced AutoMLx Configuration</a>\n",
    "  - <a href='#custom-validation'> Use a custom validation set</a>\n",
    "  - <a href='#evaluation'> Final evaluation of the best model</a>\n",
    "\n",
    "<a id='setup'></a>\n",
    "## Setup\n",
    "\n",
    "Basic setup for the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a966f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from automlx import AutoRecommender, init\n",
    "\n",
    "# Settings for plots\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 7]\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "\n",
    "# Silence unnecessary warnings\n",
    "logging.getLogger(\"sanerec.autotuning.parameter\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize the parallelization engine of AutoMLx\n",
    "init(engine='ray', engine_opts={\"ray_setup\": {\"log_to_driver\": False}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a392aee",
   "metadata": {},
   "source": [
    "<a id='load-data'></a>\n",
    "## Load Movielens 100k data\n",
    "Movielens 100k dataset is one of the most common public datasets for movie recommendation. It contains 100k ratings from about 1k users on 1.6k movies, some information about user demographic, and additional movie characteristics. For more information about this dataset, you can visit the [Movielens website](https://grouplens.org/datasets/movielens/100k/).\n",
    "\n",
    "In this demo, we use the ratings to train a movie recommendation model, exploiting AutoMLx to find the best recommendation model and hyperparameters to use in terms of recommendation accuracy.\n",
    "Therefore, we start retrieving and loading the ratings data of the Movielens 100k dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbdd65",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_ipython().system(' wget https://files.grouplens.org/datasets/movielens/ml-100k/u.data --no-check-certificate -q -O ./ml100k_interactions.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = pd.read_csv(\n",
    "    \"./ml100k_interactions.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"],\n",
    ")\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f8be9",
   "metadata": {},
   "source": [
    "In order to be used for the recommendation task, the data must have a timestamp column that is used to infer the temporal order of the samples. We also require to set the timestamp column as index of the dataframes used in our AutoML pipelines.\n",
    "\n",
    "Movielens contains a `timestamp` column that contains the time when a rating was given, so we set it as index of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21172cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = dataset.set_index(\"timestamp\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba028e7",
   "metadata": {},
   "source": [
    "<a id='coltypes'></a>\n",
    "### Define types of columns in the dataframe\n",
    "\n",
    "The recommendation task requires to define the two main entities involved in the recommendation:\n",
    "- the `recommendation`, which represents the entity type that is going to be recommended;\n",
    "- the `recommendation_subject`, which represents the entity type that receives the recommendation.\n",
    "\n",
    "For this reason, AutoML requires to indicate what are the columns in the dataset that refer to these two concepts, and, in particular, the two columns that contain their unique identifiers.\n",
    "\n",
    "In our demo we want to recommend movies (`recommendation`), identified by the `movie_id` column, to users (`recommendation_subject`), identified by the `user_id` column. We declare this binding in a python dictionary that we will reuse throughout the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17408b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "col_types = {\"movie_id\": \"recommendation\", \"user_id\": \"recommendation_subject\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454f82b",
   "metadata": {},
   "source": [
    "<a id='dataset-split'></a>\n",
    "## Splitting the dataset\n",
    "\n",
    "We split the dataset into training and test datasets using a leave-last-out technique.\n",
    "The training set will be used to create a Machine Learning model using Oracle AutoMLx, and the test set will be used to evaluate the model's performance on unseen data.\n",
    "\n",
    "The leave-last-out splitting technique consists in keeping in the test set only the last data sample, as determined by its timestamp, for each `recommendation_subject` (user in this case). All the other samples form the training set. This corresponds to the common next item recommendation use case, where given the history of all the past data concerning a `recommendation_subject` in the training set, we want to predict what should be recommended next to the same subject, and check if it corresponds to the actual sample in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942dc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data, test_data = AutoRecommender.train_test_split(data=dataset, col_types=col_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4e439",
   "metadata": {},
   "source": [
    "<a id='automl'></a>\n",
    "# AutoML\n",
    "\n",
    "<a id='provider'></a>\n",
    "## Create an instance of Oracle AutoMLx\n",
    "\n",
    "The Oracle AutoMLx solution provides a pipeline that automatically finds a tuned model given a prediction task and a training dataset. In particular, it allows finding a tuned model for any supervised prediction task, for example, classification or regression where the target can be binary, categorical or real-valued.\n",
    "\n",
    "In this demo we want a model that performs a recommendation task, so we create a pipeline of type `AutoRecommender`, and we configure it with default parameters. You can find the complete list of all the available parameters and their meaning in our documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "automl_pipeline = AutoRecommender().configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de63ad",
   "metadata": {},
   "source": [
    "<a id='default'></a>\n",
    "## Train a model using AutoMLx\n",
    "\n",
    "The training data is passed to the `fit()` function which executes the model selection and hyperparameter tuning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "automl_pipeline = automl_pipeline.fit(data=training_data, col_types=col_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f9112",
   "metadata": {},
   "source": [
    "<a id='recommending'></a>\n",
    "## Generate recommendations\n",
    "\n",
    "Once the AutoML pipeline is completed, we predict 5 recommendations for a random user in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "recommendation_subjects = test_data.sample(1)[['user_id']]\n",
    "automl_pipeline.predict(subjects=recommendation_subjects, n_recommendations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c9473e",
   "metadata": {},
   "source": [
    "<a id='analyze'></a>\n",
    "## Analyze the AutoMLx optimization process\n",
    "\n",
    "During the Oracle AutoMLx process for recommendation, a summary of the optimization process is logged, containing:\n",
    "- Information about the training data.\n",
    "- Information about the AutoMLx Pipeline, such as:\n",
    "    - Selected algorithm that was the best choice for this data;\n",
    "    - Selected hyperparameters for the selected algorithm.\n",
    "\n",
    "AutoMLx provides a `print_summary` API to output all the different trials performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "automl_pipeline.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44969dd1",
   "metadata": {},
   "source": [
    "We also provide the capability to visualize the results of each stage of the AutoMLx pipeline.\n",
    "\n",
    "<a id='algorithm-selection'></a>\n",
    "### Algorithm Selection\n",
    "\n",
    "The plot below shows the scores predicted by Algorithm Selection for each algorithm. The horizontal line shows the average score across all algorithms. Algorithms below the line are colored turquoise, whereas those with a score higher than the mean are colored teal. The selected algorithm is in orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3860ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_model_selection_scores(_pipeline):\n",
    "    # Each trial is a row in a dataframe that contains\n",
    "    # Algorithm, Number of Samples, Number of Features, Hyperparameters, Score, Runtime, Memory Usage, Step as features\n",
    "    trials = _pipeline.completed_trials_summary_[\n",
    "        _pipeline.completed_trials_summary_[\"Step\"].str.contains(\"Model Selection\")\n",
    "    ]\n",
    "    name_of_score_column = f\"Score ({_pipeline._inferred_score_metric[0].name})\"\n",
    "    trials.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    trials.dropna(subset=[name_of_score_column], inplace=True)\n",
    "    scores = trials[name_of_score_column].tolist()\n",
    "    models = trials[\"Algorithm\"].tolist()\n",
    "    \n",
    "    y_margin = 0.10 * (max(scores) - min(scores))\n",
    "    s = pd.Series(scores, index=models).sort_values(ascending=False)\n",
    "    \n",
    "    colors = []\n",
    "    for f in s.keys():\n",
    "        if f.strip() == _pipeline.selected_model_.strip():\n",
    "            colors.append(\"orange\")\n",
    "        elif s[f] >= s.mean():\n",
    "            colors.append(\"teal\")\n",
    "        else:\n",
    "            colors.append(\"turquoise\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.set_title(\"Algorithm Selection Trials\")\n",
    "    ax.set_ylim(min(scores) - y_margin, max(scores) + y_margin)\n",
    "    ax.set_ylabel(\"Hit Rate\")\n",
    "    s.plot.bar(ax=ax, color=colors, edgecolor=\"black\")\n",
    "    ax.axhline(y=s.mean(), color=\"black\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_model_selection_scores(automl_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227bf75",
   "metadata": {},
   "source": [
    "<a id='hyperparameter-tuning'></a>\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter Tuning is the last stage of the AutoMLx pipeline, and focuses on improving the chosen algorithm's score on the dataset. We use a novel iterative algorithm to search across many hyperparameter dimensions, and converge automatically when optimal hyperparameters are identified. Each trial represents a particular hyperparameter configuration for the selected model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_hp_tuning_scores(_pipeline):\n",
    "    # Each trial is a row in a dataframe that contains\n",
    "    # Algorithm, Number of Samples, Number of Features, Hyperparameters, Score, Runtime, Memory Usage, Step as features\n",
    "    trials = _pipeline.completed_trials_summary_[\n",
    "        _pipeline.completed_trials_summary_[\"Step\"].str.contains(\"Model Tuning\")\n",
    "    ]\n",
    "    name_of_score_column = f\"Score ({_pipeline._inferred_score_metric[0].name})\"\n",
    "    trials.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    trials.dropna(subset=[name_of_score_column], inplace=True)\n",
    "    trials.drop(trials[trials[\"Finished\"] == -1].index, inplace=True)\n",
    "    trials[\"Finished\"] = trials[\"Finished\"].apply(\n",
    "        lambda x: time.mktime(datetime.datetime.strptime(x, \"%a %b %d %H:%M:%S %Y\").timetuple())\n",
    "    )\n",
    "    trials.sort_values(by=[\"Finished\"], ascending=True, inplace=True)\n",
    "    scores = trials[name_of_score_column].tolist()\n",
    "    score = []\n",
    "    score.append(scores[0])\n",
    "    for i in range(1, len(scores)):\n",
    "        if scores[i] >= score[i - 1]:\n",
    "            score.append(scores[i])\n",
    "        else:\n",
    "            score.append(score[i - 1])\n",
    "    y_margin = 0.10 * (max(score) - min(score))\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.set_title(\"Hyperparameter Tuning Trials\")\n",
    "    ax.set_xlabel(\"Iteration $n$\")\n",
    "    ax.set_ylabel(\"Hit Rate\")\n",
    "    ax.grid(color=\"g\", linestyle=\"-\", linewidth=0.1)\n",
    "    ax.set_ylim(min(score) - y_margin, max(score) + y_margin)\n",
    "    ax.plot(range(1, len(trials) + 1), score, \"k:\", marker=\"s\", color=\"teal\", markersize=3)\n",
    "    plt.show()\n",
    "\n",
    "plot_hp_tuning_scores(automl_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e52185",
   "metadata": {},
   "source": [
    "<a id='modellist'></a>\n",
    "## Advanced AutoMLx Configuration\n",
    "\n",
    "You can also configure the AutoRecommender pipeline with suitable parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd62107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "custom_pipeline = AutoRecommender().configure(\n",
    "    model_list=[  # Specify the models you want the AutoMLx to consider\n",
    "        \"ItemKNNRecommender\",\n",
    "        \"AlsRecommender\",\n",
    "        \"BprRecommender\",\n",
    "    ],\n",
    "    n_algos_tuned=2,  # Choose how many models to tune\n",
    "    search_space={  # You can specify the hyperparameters and ranges we search for each model\n",
    "        \"ItemKNNRecommender\": {\"num_of_neighbors\": {\"range\": [10, 30], \"type\": \"continuous\"}}\n",
    "    },\n",
    "    max_tuning_trials=20,  # The maximum number of tuning trials. Can be integer or Dict (max number for each model)\n",
    "    score_metric=\"recall\",  # Any of the metrics available, see the documentation for a list of supported values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4b708",
   "metadata": {},
   "source": [
    "<a id='custom-validation'></a>\n",
    "## Use a custom validation set\n",
    "\n",
    "You can specify a custom validation set that you want AutoMLx to use to evaluate the quality of models and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1937eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_data, validation_data = AutoRecommender.train_test_split(data=training_data, col_types=col_types)\n",
    "\n",
    "\n",
    "# We run again the AutoML pipeline with the custom training/validation split we just created, and some advanced settings that we can specify directly in the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca34c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "custom_pipeline = custom_pipeline.fit(\n",
    "    training_data,\n",
    "    col_types,\n",
    "    validation_data,\n",
    "    time_budget=30,  # Specify time budget in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aee01c",
   "metadata": {},
   "source": [
    "Now that the custom AutoML pipeline is completed, we can generate recommendations.\n",
    "Note that the pipeline's `recommend` method is equivalent to `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f21d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "custom_pipeline.recommend(subjects=recommendation_subjects, n_recommendations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fd542",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "## Final evaluation of the best model\n",
    "\n",
    "Finally, we evaluate the best model found on the test data we have. If no metric is specified, the pipeline computes the score using the same metric used to run the Hyperparameter Tuning, which in this case is the Recall, as we defined at pipeline creation.\n",
    "\n",
    "In this example, instead, we ask the pipeline to perform the evaluation using Normalized Discounted Cumulative Gain (NDCG), a common ranking metric. Our online documentation provides the list of the available metrics and how they are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6dece",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_ipython().run_line_magic('precision', '4')\n",
    "custom_pipeline.score(data=test_data, score_metric=\"ndcg\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
